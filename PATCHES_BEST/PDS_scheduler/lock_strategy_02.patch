diff --git a/kernel/sched/bfs.c b/kernel/sched/bfs.c
index 492bf08e5536..bae240291eb7 100644
--- a/kernel/sched/bfs.c
+++ b/kernel/sched/bfs.c
@@ -238,17 +238,10 @@ static inline struct rq
 	struct rq *rq;
 	for (;;) {
 		rq = task_rq(p);
-		if (p->on_cpu) {
+		if (p->on_cpu || task_on_rq_queued(p)) {
 			raw_spin_lock(&rq->lock);
-			if (likely(p->on_cpu && rq == task_rq(p))) {
-				*plock = &rq->lock;
-				return rq;
-			}
-			raw_spin_unlock(&rq->lock);
-		} else if (task_queued(p)) {
-			raw_spin_lock(&rq->lock);
-			if (likely(!p->on_cpu && task_queued(p) &&
-				   rq == task_rq(p))) {
+			if (likely((p->on_cpu || task_on_rq_queued(p))
+				   && rq == task_rq(p))) {
 				*plock = &rq->lock;
 				return rq;
 			}
@@ -274,24 +267,17 @@ struct rq
 	struct rq *rq;
 	for (;;) {
 		rq = task_rq(p);
-		if (p->on_cpu) {
+		if (p->on_cpu || task_on_rq_queued(p)) {
 			raw_spin_lock_irqsave(&rq->lock, *flags);
-			if (likely(p->on_cpu && rq == task_rq(p))) {
-				*plock = &rq->lock;
-				return rq;
-			}
-			raw_spin_unlock_irqrestore(&rq->lock, *flags);
-		} else if (task_queued(p)) {
-			raw_spin_lock_irqsave(&rq->lock, *flags);
-			if (likely(!p->on_cpu && task_queued(p) &&
-				   rq == task_rq(p))) {
+			if (likely((p->on_cpu || task_on_rq_queued(p))
+				   && rq == task_rq(p))) {
 				*plock = &rq->lock;
 				return rq;
 			}
 			raw_spin_unlock_irqrestore(&rq->lock, *flags);
 		} else {
 			raw_spin_lock_irqsave(&p->pi_lock, *flags);
-			if (likely(!p->on_cpu && !task_queued(p) &&
+			if (likely(!p->on_cpu && !task_on_rq_queued(p) &&
 				   rq == task_rq(p))) {
 				*plock = &p->pi_lock;
 				return rq;
@@ -1733,9 +1719,7 @@ static inline void ttwu_activate(struct task_struct *p, struct rq *rq)
 	activate_task(p, rq);
 
 	/*
-	 * if a worker is waking up, notify workqueue. Note that on BFS, we
-	 * don't really know what CPU it will be, so we fake it for
-	 * wq_worker_waking_up :/
+	 * if a worker is waking up, notify workqueue.
 	 */
 	if (p->flags & PF_WQ_WORKER)
 		wq_worker_waking_up(p, cpu_of(rq));
